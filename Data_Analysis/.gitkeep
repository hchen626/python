There's going to be some overlap with Machine Learning Course

Lab 1 - Review
	- Read data
	- Dataframe - head() and tail()

- Add headers to dataframe
    df = pd.read_csv(other_path, header=None)
    headers=[your_list]
    df.columns = headers
	
-Cleanup
    -Replace essentially null values
	    df1=df.replace('?',NaN)
	-Statistical summary on dataframe
		dataframe.describe()

Lab 2 - Data Wrangling
	- Missing data
		df.isnull()
	- Replace NaN with avg_value
		avg_bore=df['bore'].astype('float').mean(axis=0)
		df["bore"].replace(np.nan, avg_bore, inplace=True)
	- Dataframe info
		df.dtypes
	- Dataframe operations
		df.rename(columns={"highway-mpg":"highway-L/100km"}, inplace=True) -- rename columns
	- Binning - like grouping rows evenly
		 bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)	
		 group_names = ['Low', 'Medium', 'High']
		 df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
	- Create indicator variables and assign it to dataframe - It's like transposing a column
		 dummy_variable_2 = pd.get_dummies(df['aspiration'])
		 df = pd.concat([df, dummy_variable_2], axis=1)
		 df.drop('aspiration', axis = 1, inplace=True)
	 
Lab 3 - Data Analysis with Python - Introduce Seaborn
	- Correlation Matrix on dataframe
		df.corr()
	- Plot Regression Plot Fast
		sns.regplot(x="engine-size", y="price", data=df)
		plt.ylim(0,)

	- Box plot
		sns.boxplot(x="body-style", y="price", data=df)

	- Get Frequency Count of a DF column
		df['drive-wheels'].value_counts()
		df['drive-wheels'].value_counts().to_frame()

	- Go over grouping and pivoting - allows you to perform operations on the groups
		df_gptest = df[['drive-wheels','body-style','price']]
		grouped_test1 = df_gptest.groupby(['drive-wheels','body-style'],as_index=False).mean()
		grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')
		
	- Correlation - by leverging SCIPY
		- Pearson_coef and P-Value
		   pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])
		   Pearson Correlation = measures linear dipendence between two variables X and Y
		   P-value is the probability value that the corrrelation between two variables is statistically signficant 

- ANOVA
	- Test whether there are significant differences between the means of two or more groups
	- F-test score
	- P-Value
	- Suppose you have three types of groups and want to understand if there is a clear separation in terms of pricing
	- Look at three at the same time
	- f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  
	- Rerun, but only look at two at time
		- fwd and rwd
		- 4wd and rwd
		- 4wd and fwd
	f_val, p_val = stats.f_oneway(grouped_test2.get_group('4wd')['price'], grouped_test2.get_group('rwd')['price']) 

Lab 4 - Review Model Development
    - LinearRegression()
	  lm = LinearRegression()
	  lm.intercept_
	  lm.coef_
	- For a linear model, you can plot residual plot that represents a the difference between observed value (y) and predicted value (Yhat)  
    - It goes over MLR and SLR, which you could find 
	- It goes over a little bit on PolynomialFeatures
	  yhat = a + b_{1}X_{1} + b_{2}X_{2} +  b_{3}X_{1}X_{2} + b_{4}X_{1}^{2} +  b_{5}X_{2}^{2}
	  You need to transform the feature set and run linear regresion object on the transformed 
	  
	  pr = PolynomialFeatures(degree=5)
	  x_train_pr = pr.fit_transform(x_train[['horsepower']])
	  x_test_pr = pr.fit_transform(x_test[['horsepower']])
	  poly = LinearRegression()
	  poly.fit(x_train_pr, y_train)
	  yhat = poly.predict(x_test_pr)
      yhat[0:5]
	  print("Predicted values:", yhat[0:4])
	  print("True values:", y_test[0:4].values)
	  poly.score(x_test_pr, y_test)
	  
	- Something really useful is Pipeline
	
	You can feed in your inputs to process your data.
	Quickly Scale and Model
	
	from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
	Input=[('scale',StandardScaler()), 
           ('polynomial', PolynomialFeatures(include_bias=False)), 
           ('model',LinearRegression())
          ]
	pipe=Pipeline(Input)
    pipe
	pipe.fit(Z,y)
	ypipe=pipe.predict(Z)
    ypipe[0:4]
	   
 	
Shortcuts for Model Evalution metrics
    - MSE
	from sklearn.metrics import mean_squared_error
	mse = mean_squared_error(df['price'], Yhat)
	
	R-squared
	- There is a function you can apply on LinearRegression object to get R-squared
	print('The R-square is: ', lm.score(X, Y))

	
	
Lab 5 - Review Model Evalution and Refinement
    - K-Fold cross validation
	- looks we feed in the model object, feature set, target variable, number of folds
	
		from sklearn.linear_model import LinearRegression
		lre=LinearRegression()
		
		from sklearn.model_selection import cross_val_score
		Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)
		Rcross #displays the default scoring, which is R-squared
		print("The mean of the folds are", Rcross.mean(), "and the standard deviation is" , Rcross.std())
	
	- Goes over Ridge Regression
	   - Ridge Regression Prevents overfitting
	   - Ridge Regression controls the magnitude of these polynomials by introducing the parameter Alpha.
	   - Increasing alpha could lead to more underfitting
	- Use Gridsearch - in short, it helps find the best alpha to use
	  The object iteratively tries different alphas and compares  the MSE or R-Squared 
	  to find the best alpha to use in the Ridge Regression object
